# Data Model: Module 5: Vision-Language-Action (VLA)

**Branch**: `001-vla-capstone-module` | **Date**: 2025-12-13 | **Plan**: `specs/001-vla-capstone-module/plan.md`

## Conceptual Entities for VLA Paradigm

This module focuses on conceptual understanding rather than a traditional software data model with persistent storage and relational schemas. The entities below represent the key information flows and components discussed in the VLA textbook content.

### Entity: Voice Command
-   **Description**: Natural language spoken input from a user, intended to instruct the autonomous humanoid.
-   **Attributes (Conceptual)**:
    -   `audio_stream`: Raw audio data from microphone.
    -   `timestamp`: Time of command utterance.
    -   `speaker_identity`: (Optional) Identification of the speaker.
-   **Relationships**: Transformed into `Text Command` by `Speech Recognition Model`.

### Entity: Text Command
-   **Description**: The transcribed text representation of a `Voice Command`. This is the input to the cognitive planning LLM.
-   **Attributes (Conceptual)**:
    -   `text_string`: The transcribed natural language sentence or phrase.
    -   `language`: Language of the command.
-   **Relationships**: Derived from `Voice Command`. Input to `Large Language Model`.

### Entity: Large Language Model (LLM)
-   **Description**: An AI model responsible for cognitive planning, translating high-level natural language `Text Commands` into a sequence of low-level `ROS 2 Action Sequence` based on contextual information.
-   **Attributes (Conceptual)**:
    -   `model_type`: Specific LLM used (e.g., GPT-4, Gemini).
    -   `context_window`: Maximum input token length.
    -   `prompt_template`: Structure and content of the instruction given to the LLM for planning.
-   **Relationships**: Processes `Text Command` and `Computer Vision System` context to generate `ROS 2 Action Sequence`.

### Entity: Speech Recognition Model (e.g., OpenAI Whisper)
-   **Description**: A component that converts raw `Voice Command` audio data into `Text Command`.
-   **Attributes (Conceptual)**:
    -   `model_name`: Specific speech recognition model.
    -   `accuracy_rate`: Performance metric for transcription.
    -   `latency`: Time taken for transcription.
-   **Relationships**: Transforms `Voice Command` into `Text Command`.

### Entity: ROS 2 System
-   **Description**: The distributed robotics operating system that orchestrates the robot's hardware and software components, executing `ROS 2 Action Sequence`.
-   **Attributes (Conceptual)**:
    -   `nodes`: Individual processes within the ROS 2 graph.
    -   `topics`: Communication channels for data exchange.
    -   `services`: Request/reply communication mechanisms.
    -   `actions`: Long-running, goal-oriented tasks.
-   **Relationships**: Receives and executes `ROS 2 Action Sequence`. Interacts with `Autonomous Humanoid`.

### Entity: Computer Vision System (from Module 4's VSLAM)
-   **Description**: Provides real-time environmental context and object identification, feeding this information into the LLM's cognitive planning process.
-   **Attributes (Conceptual)**:
    -   `object_detections`: List of identified objects with bounding boxes/poses.
    -   `semantic_map`: Environmental understanding.
    -   `localization_data`: Robot's pose in the environment.
-   **Relationships**: Provides context to `Large Language Model`.

### Entity: ROS 2 Action Sequence
-   **Description**: A structured, ordered list of low-level, executable commands or goals for the `Autonomous Humanoid`, generated by the LLM.
-   **Attributes (Conceptual)**:
    -   `actions`: Ordered list of discrete robot actions (e.g., `navigate(target_pose)`, `grasp(object_id)`, `identify(object_class)`).
    -   `parameters`: Arguments for each action.
    -   `pre_conditions`: Conditions that must be met before an action can execute.
    -   `post_conditions`: Expected state after action execution.
-   **Relationships**: Generated by `Large Language Model`. Executed by `ROS 2 System`.

### Entity: Autonomous Humanoid
-   **Description**: The physical robotic platform that executes commands and interacts with the physical environment.
-   **Attributes (Conceptual)**:
    -   `sensors`: Onboard sensors (cameras, LiDAR, IMU).
    -   `actuators`: Motors, grippers, etc.
    -   `kinematics_model`: Mathematical description of robot motion.
-   **Relationships**: Executes actions via `ROS 2 System`. Provides sensor data that informs `Computer Vision System`.

## Relationships & Data Flow (High-Level)

1.  `Voice Command` -> `Speech Recognition Model` -> `Text Command`
2.  `Text Command` + `Computer Vision System` context -> `Large Language Model`
3.  `Large Language Model` -> `ROS 2 Action Sequence`
4.  `ROS 2 Action Sequence` -> `ROS 2 System` -> `Autonomous Humanoid`
5.  `Autonomous Humanoid` (sensors) -> `Computer Vision System` (feedback loop)
