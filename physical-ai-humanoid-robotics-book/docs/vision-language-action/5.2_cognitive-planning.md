---
id: 5.2-cognitive-planning
title: '5.2 Cognitive Planning: LLM-to-ROS 2'
---

# 5.2 Cognitive Planning: LLM-to-ROS 2 Action Translation

## Objective

Upon completing this chapter, you will be able to:
- Define "cognitive planning" in the context of robotic control with Large Language Models (LLMs).
- Explain how LLMs translate high-level natural language commands into executable ROS 2 action sequences.
- Understand the critical role of prompt engineering in guiding LLMs for robust robotic task planning.
- Analyze the feedback mechanisms that allow LLMs to adapt plans based on real-world sensor data.

## Conceptual Explanation: The Robot's "Brain"

With a voice command transcribed into text, the next step in the Vision-Language-Action (VLA) pipeline is **Cognitive Planning**. This is where the robot's "brain," powered by an LLM, transforms an abstract goal (e.g., "Get me a drink") into a concrete sequence of low-level robotic actions that the ROS 2 system can execute.

Traditionally, robotic planning required complex, handcrafted algorithms that struggled with the ambiguity of human language. LLMs, with their vast world knowledge and reasoning capabilities, provide a powerful and flexible alternative for this cognitive layer.

### How LLMs Power Robotic Planning

1.  **Natural Language Understanding**: They interpret the nuances of human language, including context and implied meanings.
2.  **World Knowledge**: They use general knowledge to make informed inferences (e.g., knowing a "drink" is likely in the "kitchen").
3.  **Task Decomposition**: They can break down a high-level goal into a logical sequence of smaller sub-tasks.
4.  **Function Calling (Tool Use)**: Modern LLMs can be given a set of "tools" that correspond to the robot's physical abilities (e.g., ROS 2 actions like `navigate_to()` or `grasp_object()`). The LLM's job is to select the right tool with the right arguments.

For an LLM to generate a feasible plan, it needs **contextual grounding** from the robot's perception system. This includes the robot's current location, a map of the environment, and a list of detected objects.

## Technical Deep Dive: Prompt Engineering for Robotics

**Prompt Engineering** is the practice of carefully crafting the input to an LLM to get the desired output. For robotics, a good prompt is essential for safety and reliability.

### Core Components of a Robotic Planning Prompt

A typical prompt for cognitive planning includes:
-   **System Role**: "You are a helpful robotic assistant."
-   **User Command**: The transcribed high-level instruction.
-   **Environmental Context**: A snapshot of the robot's state and a list of detected objects.
-   **Available Tools**: A clear definition of the ROS 2 actions the robot can perform.
-   **Output Format**: Instructions to provide the plan in a machine-readable format, like JSON.

### Example Flow: LLM-based Cognitive Planning

The following diagram illustrates the cognitive planning loop:

```mermaid
graph TD
    A[Text Command] --> B{LLM Cognitive Planner};
    C[Environmental Context (Vision)] --> B;
    B -- Creates Plan --> D[ROS 2 Action Sequence];
    D -- Executes Step-by-Step --> E[Robot Hardware];
    E -- Sensor Feedback --> C;
    D -- Execution Status --> B;
```

### Python Pseudocode: Generating a Plan

This pseudocode shows how to interact with an LLM to generate a structured plan and then execute it. The key is providing the LLM with a clear, detailed prompt.

```python
import json

# Mocks for demonstration purposes
class LLMClient:
    def generate(self, prompt):
        # In a real system, this would be an API call to an LLM.
        # Here, we return a pre-defined JSON string for a sample command.
        print("--- LLM PROMPT ---")
        print(prompt)
        print("--- END PROMPT ---")
        
        # This is the kind of clean JSON response we expect from the LLM.
        # It is a string that can be safely parsed.
        example_plan_json = """
        [
            {
                "tool": "navigate_to",
                "args": {"location_name": "kitchen_counter"}
            },
            {
                "tool": "grasp_object",
                "args": {"object_id": "apple_01"}
            },
            {
                "tool": "navigate_to",
                "args": {"location_name": "user_location"}
            },
            {
                "tool": "report_success",
                "args": {"message": "I have brought you the apple."}
            }
        ]
        """
        return example_plan_json

class RobotActions:
    def navigate_to(self, location_name):
        print(f"ACTION: Navigating to {location_name}...")
        return True # Simulate success

    def grasp_object(self, object_id):
        print(f"ACTION: Grasping object {object_id}...")
        return True # Simulate success
    
    def report_success(self, message):
        print(f"STATUS: {message}")

    def report_failure(self, reason):
        print(f"ERROR: {reason}")

def generate_and_execute_plan(command, detected_objects):
    """
    Generates and executes a plan from a text command.
    """
    llm = LLMClient()
    robot = RobotActions()

    # 1. Define the tools available to the LLM.
    available_tools_desc = [
        "navigate_to(location_name: str) - Moves to a predefined location.",
        "grasp_object(object_id: str) - Grasps an object by its ID.",
        "report_success(message: str) - Reports successful task completion.",
        "report_failure(reason: str) - Reports a failure with a reason."
    ]
    
    # 2. Format the context for the prompt.
    objects_str = ", ".join([str(obj) for obj in detected_objects])
    tools_str = "\n".join(available_tools_desc)

    # 3. Construct the prompt using simple string formatting.
    # This avoids complex f-string syntax that can confuse MDX parsers.
    prompt = (
        "You are an autonomous robot planner.\n"
        "Your goal is to create a JSON-formatted plan to achieve the user's command.\n\n"
        "## User Command:\n"
        f"{command}\n\n"
        "## Context:\n"
        f"- Detected Objects: [{objects_str}]\n\n"
        "## Available Tools:\n"
        f"{tools_str}\n\n"
        "## Instructions:\n"
        "Respond with a valid JSON array of tool calls. Do not include any other text.\n"
        "Example: [{'tool': 'navigate_to', 'args': {'location_name': 'kitchen'}}]"
    )

    # 4. Get the plan from the LLM.
    try:
        response_str = llm.generate(prompt)
        action_plan = json.loads(response_str)
    except Exception as e:
        robot.report_failure(f"Failed to get or parse LLM plan: {e}")
        return

    # 5. Execute the plan.
    print("\n>>> EXECUTING PLAN <<<")
    for action in action_plan:
        tool_name = action.get("tool")
        args = action.get("args", {})
        
        try:
            action_func = getattr(robot, tool_name)
            action_func(**args)
        except Exception as e:
            robot.report_failure(f"Action '{tool_name}' failed: {e}")
            break # Stop execution on failure

# Example Usage
if __name__ == '__main__':
    user_command = "Please get the apple from the kitchen counter and bring it to me."
    current_objects = [
        {"name": "apple", "id": "apple_01", "location": "kitchen_counter"}
    ]
    generate_and_execute_plan(user_command, current_objects)

```

## Summary

- **Cognitive Planning** uses LLMs to bridge the gap between high-level human intent and low-level robotic actions.
- **Prompt Engineering** is the key to ensuring LLMs produce safe, reliable, and effective plans.
- **Context is King**: Grounding the LLM with real-time sensor data is essential for generating plans that are physically feasible.

This chapter has provided a conceptual and technical overview of using LLMs for cognitive planning. The next chapter will build on this to integrate these plans into our capstone project.
